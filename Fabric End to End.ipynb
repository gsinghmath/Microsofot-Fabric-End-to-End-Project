{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bb86fb7-520e-4d2c-9c6d-19a33d06b31f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# European Dynamic Energy Prices Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163821f-baff-45f6-bfd0-92253b995523",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "<a id=\"0\"></a>\n",
    "# Table of Contents\n",
    "\n",
    "1. [Tech Stack](#ts)\n",
    "2. [Goals](#goal)\n",
    "3. [Architecture](#arch)\n",
    "4. [Scraping Data](#sd)\n",
    "5. [Storing in Delta Tables](#delta)\n",
    "6. [Scheduling](#schd)\n",
    "7. [Transformations in DBT](#dbt)\n",
    "8. [Power BI Report](#pbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d314a76-0d5e-49eb-9fdd-3191e281ec96",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id=\"ts\"></a>\n",
    "# Tech Stack\n",
    "\n",
    "\n",
    "|Use Case|Local| Fabric Equivalent | \n",
    "|:------:| :---------: |  :-------: | \n",
    "|Programming Language|python|python|\n",
    "|Web Scraping|Beautiful Soup 4| Beautiful Soup 4 |\n",
    "|Data Storage| DuckDB\\| Delta Tables| OneLake\\| Lakehouse\\| Delta Tables|\n",
    "|Data Transformations|dbt|dbt |\n",
    "|Visualization|Power BI Desktop| Power BI Desktop/|Power BI Service |\n",
    "|Scheduling| Airflow | Pipeline|\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "Other Fabric Components that would have been used:<br>\n",
    "1. <b>Notebook:</b> for running PySpark jobs<br>\n",
    "2. <b>Lakehouse SQL endpoint:</b> for storing results from dbt runs. </div>\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bd30c3-de1b-4685-8724-51c2fd283bb6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id=\"goal\"></a>\n",
    "# Goals\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "    The origial idea was to build a project on <b>microsoft Fabric</b> to apply its capabilites for a typical end to end project.\n",
    "\n",
    "\n",
    "But currently Microsoft Fabric has Issues and it would not allow me purchase/trial any capacity no matter what options I tried.\n",
    "\n",
    "</div>\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "<a href=\"https://www.skool.com/microsoft-fabric/problems-starting-a-new-fabric-free-trials-an-update\" style=\"color:blue; text-decoration:underline\"\">ISSUE DETAILS HERE</a>\n",
    "</br>\n",
    "So I decided to build this project locally for now and maybe apply Fabric features in the future once the issues are sorted out.\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "Since European markets moved to **dynamic energy prices** which brings challenges, but also lots of opportunities. By analyzing the prices, one can optimize your energy consumption and save money. The flexibility and options improve a lot with the installation of a home battery. \n",
    "\n",
    "In this porject will be scraping Epex Spot (European Energy Exchange) day-ahead energy pricing data. Energy companies buy and sell energy on this exchange. The price of energy is announced one day in advance. The price can even become negative when there will be too much energy on the grid.\n",
    "\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "The goals of this project are as follows:\n",
    "</br>\n",
    "1. Web Scrape data from <a href=\"https://www.epexspot.com/\" style=\"color:blue; text-decoration:underline\">Epex Spot</a> (European Energy Exchange) day-ahead energy pricing data. Energy companies buy and sell energy on this exchange. The price of energy is announced one day in advance. The price can even become negative when there will be too much energy on the grid (e.g. it’s sunnier and windier than expected and some energy plants cannot easily scale down capacity)\n",
    "</br>\n",
    "Unfortunately, Epec Spot website does not offer a free API to get the data, so I will use <b>BeautifulSoup4</b> to scrape this data.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src='./images/fabric_project/epexspot.png' stlye='display: block; margin: 0 auto' alt=\"Epex Spot\"  width=\"700\" height=\"600\" >\n",
    "</br>\n",
    "2. Store it in <b>Delta Tables</b> locally\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;At its very core, a <b>Delta Tables</b> is a bunch of Parquet files stored in a data lake, with a special transaction log that keeps track of all the changes made to the data. This log is the secret sauce that makes Delta Tables so powerful. It enables cool features like ACID transactions, data versioning, schema enforcement, and performance optimizations.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "    I will use <b>Duck DB</b> as the local database for reading and writing to local <b>Delta Tables</b>\n",
    "</div>\n",
    "<a href=\"https://www.chaosgenius.io/blog/delta-table/\" style=\"color:blue; text-decoration:underline\">SOURCE</a>\n",
    "</br>\n",
    "3. Use <b>dbt</b> for transforming data for creating a report in Power BI\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dbt is a tool for analytics engineers where you can use SQL with Jinja templating to make that SQL easier to use and add functionalities. By using dbt, we can leverage a lot of best practices from software engineering in our analytics projects.\n",
    "</br>\n",
    "4. Use tranformed data in <b>Power BI</b> for creating a Report\n",
    "</div>\n",
    "\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b8d9cd-b4d7-480f-9869-ce97dc433240",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id='arch'></a>\n",
    "# Architecture\n",
    "\n",
    "***\n",
    "\n",
    "If it were the project in **Microsoft Fabric**, then below was the Architecture I planned\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src='./images/fabric_project/architecture-1.png' stlye='display: block; margin: 0 auto' alt=\"Micrsoft Fabric Project Architecture\"  width=\"700\" height=\"600\" >\n",
    "\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dcf3fd-ba67-4f08-b673-dde7092e27ee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id=\"sd\"></a>\n",
    "# Scraping Data\n",
    "***\n",
    "###  1. Beautifulsoup4 Installion\n",
    "\n",
    "##### Local Environment\n",
    "***\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "\n",
    "<ol>\n",
    "    <li>Install Python. Since I planned to use PySpark, I installed version 3.10 as that is the one seems to be working on my machine. </li>\n",
    "    <li>Create a Virtual Env</li>\n",
    "    <li>\n",
    "        Install Beautifulsoup4\n",
    "        <br>\n",
    "        <code>pip install beautifulsoup4</code>\n",
    "    </li>\n",
    "   \n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "##### Steps I would have followed in Fabric\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "    Since I would use <b>beautifulsoup4</b> Python package in the Spark runtime environment of our Fabric Workspace, I needed to install it first. \n",
    "    <ol>\n",
    "        <li>This can be done by going to Fabric Workspace settings and then expanding the Data Engineering/Science section. </li>\n",
    "    <li>Click on </> Library management tab. </li>\n",
    "    <li>Click on the + Add from PyPI button. In the text box that appears, enter <b>beautifulsoup4</b>, pick the latest version, and click on the Apply button. This will install the package in the Workspace.</li>\n",
    "\n",
    "<br>\n",
    "<a href=\"https://fabric.guru/installing-custom-python-packages-in-fabric\" style=\"color:blue; text-decoration:underline\">\n",
    "        How to Install a Python package Microsoft Fabric\n",
    "</a>\n",
    "</br>\n",
    "The steps can be seen in the image below\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src='./images/fabric_project/install_lib_python_fabric.png' stlye='display: block; margin: 0 auto' alt=\"Micrsoft Fabric Install Python Library\"  width=\"700\" height=\"600\" >\n",
    "\n",
    "\n",
    "### 2. PySpark Installation\n",
    "***\n",
    "\n",
    "##### Local Environment\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    " Please follow steps from my other project on PySpark Installation \n",
    "    <a href=\"https://gsinghmath.pythonanywhere.com/static/Uber%20Data%20%20Analysis%20PySpark.html\" style=\"color:blue; text-decoration:underline\">here</a>\n",
    "\n",
    "I also needed to install below python packages.\n",
    "\n",
    "\n",
    "<ol>\n",
    "        <li>pandas</li>\n",
    "        <li>requests</li>\n",
    "        <li>delta-spark</li>\n",
    "        <li>pyspark</li>\n",
    "        <li>jupyterlab</li>\n",
    "</ol>\n",
    "\n",
    "    \n",
    "</div>\n",
    "    \n",
    "##### Fabric\n",
    "***\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "I would have created a new <b>Notebook</b> and followed Fabric's prompts to also create a new Lakehouse. Notebook's allow offer a PySpark enviroment.\n",
    "<br>\n",
    "<a href=\"https://www.deeplearningnerds.com/microsoft-fabric-how-to-create-a-notebook/\" style=\"color:blue; text-decoration:underline\">\n",
    "        How to Create a New Notebook Microsoft Fabric\n",
    "</a>\n",
    "\n",
    "</br>\n",
    "Above steps can be seen in Image below\n",
    "</div>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src='./images/fabric_project/notebook_fabric.png' stlye='display: block; margin: 0 auto' alt=\"Micrsoft Fabric New Notebook\"  width=\"700\" height=\"600\" >\n",
    "\n",
    "\n",
    "\n",
    "### 3. Web Scraping Code\n",
    "***\n",
    "\n",
    "##### Local Environment\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "The code in the next Cell i\n",
    "    s summarized as below:\n",
    "\n",
    "<ol>\n",
    "    <li>I created a <b>python class: EpexSpotEnergyPrices</b> to scrap data from Epex Spot website.</li>\n",
    "    <li>I submit an <b>HTTP POST request</b> to the web page which lists the day-ahead data for a chosen market in a table and the server responds with an <b>HTML</b> page. I also pass necessary parameters as expected by the webpage.</li>\n",
    "    <li>Parse the <b>HTML</b> page using <b>Beautifulsoup4</b> and extract the required table.\n",
    "    <li>I also convert the date and time to UTC, so that I don't have to worry about timezones or daylight-saving time later on. The results are stored in a <b>Pandas DataFrame</b></li>\n",
    "    <li>Then in the next cell I instantiate the class and extract the data in <b>Pandas DataFrame</b></li>\n",
    "    <li>In next cell, I use create a <b>spark session</b> and save the extracted data to <b>Delta Format and partition it by market </b> in table <b>epex_spot_prices</b></li>\n",
    "    <li>I also make sure the next run, the data is appended to the same table, since I wish to <b>schedule</b> in <b>Airflow</b>  for daily data extraction.</li>\n",
    "    <li>The resulting data with <b>market paritioning</b> would look like below</li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src='./images/fabric_project/scraping_result.png' stlye='display: block; margin: 0 auto' alt=\"Local Delta Table with Market Paritioning\"  width=\"300\" height=\"400\" >\n",
    "\n",
    "\n",
    "##### Fabric\n",
    "***\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "I would have created a function that received <b>market</b> as a parameter and scraped data for that <b>market</b> from <b>Epex Spot</b> Website. Basically modifiying the code as explained above. The result would have been saved in <b>Delta Table</b> in the <b>Lakehouse</b>.\n",
    "</div>\n",
    "\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2d1813-07e9-4729-88cf-904e6f51d54c",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-04-18T18:52:57.885698Z",
     "iopub.status.busy": "2025-04-18T18:52:57.885698Z",
     "iopub.status.idle": "2025-04-18T18:52:59.577777Z",
     "shell.execute_reply": "2025-04-18T18:52:59.577777Z",
     "shell.execute_reply.started": "2025-04-18T18:52:57.885698Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta, timezone\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "# Suppress FutureWarning messages\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class EpexSpotEnergyPrices:\n",
    "    \"\"\"Scrape data from Epex Spot website as they do not offer any API.\n",
    "       They provide Energy Auction data a day ahead, so its only possible to get data from today onwards.\n",
    "       url: https://www.epexspot.com/en/market-results\n",
    "       This has been updated recently.\n",
    "       we get the data for these markets: ['AT','BE','CH','DE-LU','DK1','DK2','FI','FR','GB','NL','NO1','NO2',\n",
    "                                           'NO3','NO4','NO5','PL','SE1','SE2','SE3','SE4']\n",
    "        Usage:\n",
    "        ep = EpexSpotEnergyPrices()\n",
    "        ep.run()\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = 'learn'\n",
    "    def _to_float(self, v: str) -> float:\n",
    "        return float(v.replace(\",\", \"\"))\n",
    "    \n",
    "    def _as_date_str(self, v: date) -> str:\n",
    "        return v.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    \n",
    "    def fetch_data(self, delivery_date: date, market_area: str) -> dict[str, Any]:\n",
    "        trading_date = delivery_date - timedelta(days=1)\n",
    "        params = {\n",
    "            \"market_area\": market_area,\n",
    "            \"trading_date\": self._as_date_str(trading_date),\n",
    "            \"delivery_date\": self._as_date_str(delivery_date),\n",
    "            'auction': 'MRC',\n",
    "            \"modality\": \"Auction\",\n",
    "            \"sub_modality\": \"DayAhead\",\n",
    "            \"product\": \"60\",\n",
    "            \"data_mode\": \"table\",\n",
    "        }\n",
    "        data = {\n",
    "            \"form_id\": \"market_data_filters_form\",\n",
    "            \"_triggering_element_name\": \"submit_js\",\n",
    "        }\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0'}\n",
    "        r = requests.post(\"https://www.epexspot.com/en/market-results\", params=params, data=data, headers=headers)\n",
    "        r.raise_for_status()\n",
    "        return str(r.content)\n",
    "    \n",
    "    def extract_table_data(self, delivery_date: datetime, data: dict[str, Any], market_area: str) -> pd.DataFrame:\n",
    "        soup = BeautifulSoup(data, features=\"html.parser\")\n",
    "    \n",
    "        try:\n",
    "            table = soup.find(\"table\", class_=\"table-01 table-length-1\")\n",
    "            body = table.tbody\n",
    "            rows = body.find_all_next(\"tr\")\n",
    "        except AttributeError:\n",
    "            return pd.DataFrame.from_records([], columns=[\"market\", \"start_time\", \"end_time\", \"buy_volume\", \"sell_volume\", \"volume\", \"price\"])\n",
    "     # no data available\n",
    "    \n",
    "        start_time = delivery_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "        # convert timezone to UTC (and adjust timestamp)\n",
    "        start_time = start_time.astimezone(timezone.utc)\n",
    "    \n",
    "        records = []\n",
    "        for row in rows:\n",
    "            end_time = start_time + timedelta(hours=1)\n",
    "            buy_volume_col = row.td\n",
    "            sell_volume_col = buy_volume_col.find_next_sibling(\"td\")\n",
    "            volume_col = sell_volume_col.find_next_sibling(\"td\")\n",
    "            price_col = volume_col.find_next_sibling(\"td\")\n",
    "            records.append(\n",
    "                (\n",
    "                    market_area,\n",
    "                    start_time,\n",
    "                    end_time,\n",
    "                    self._to_float(buy_volume_col.string),\n",
    "                    self._to_float(sell_volume_col.string),\n",
    "                    self._to_float(volume_col.string),\n",
    "                    self._to_float(price_col.string),\n",
    "                )\n",
    "            )\n",
    "            start_time = end_time\n",
    "        return pd.DataFrame.from_records(records, columns=[\"market\", \"start_time\", \"end_time\", \"buy_volume\", \"sell_volume\", \"volume\", \"price\"])\n",
    "    \n",
    "    def fetch_day(self, delivery_date: datetime, market_area) -> pd.DataFrame:\n",
    "        data = self.fetch_data(delivery_date.date(), market_area)\n",
    "        return self.extract_table_data(delivery_date, data, market_area)\n",
    "    def run(self):\n",
    "        result_df = pd.DataFrame(columns=['market', 'start_time', 'end_time', 'buy_volume', 'sell_volume',\n",
    "       'volume', 'price'])\n",
    "        markets = ['AT','BE','CH','DE-LU','DK1','DK2','FI','FR','GB','NL','NO1','NO2',\n",
    "                                           'NO3','NO4','NO5','PL','SE1','SE2','SE3','SE4']\n",
    "        for market in markets:\n",
    "            delivery_date = datetime.now() + timedelta(days=1)\n",
    "            print(\"Extracting data for\", market, 'for ', delivery_date)\n",
    "            prices = self.fetch_day(delivery_date, market_area=market)\n",
    "            result_df = pd.concat([result_df, prices], ignore_index=True)\n",
    "            \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "232de275-41a4-41cd-a998-ef48d3b36500",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:52:59.577777Z",
     "iopub.status.busy": "2025-04-18T18:52:59.577777Z",
     "iopub.status.idle": "2025-04-18T18:53:16.949619Z",
     "shell.execute_reply": "2025-04-18T18:53:16.947013Z",
     "shell.execute_reply.started": "2025-04-18T18:52:59.577777Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for AT for  2025-04-19 11:52:59.591890\n",
      "Extracting data for BE for  2025-04-19 11:53:00.752151\n",
      "Extracting data for CH for  2025-04-19 11:53:01.667114\n",
      "Extracting data for DE-LU for  2025-04-19 11:53:02.620590\n",
      "Extracting data for DK1 for  2025-04-19 11:53:03.835446\n",
      "Extracting data for DK2 for  2025-04-19 11:53:04.536127\n",
      "Extracting data for FI for  2025-04-19 11:53:05.471504\n",
      "Extracting data for FR for  2025-04-19 11:53:06.219133\n",
      "Extracting data for GB for  2025-04-19 11:53:06.945077\n",
      "Extracting data for NL for  2025-04-19 11:53:07.688589\n",
      "Extracting data for NO1 for  2025-04-19 11:53:08.448961\n",
      "Extracting data for NO2 for  2025-04-19 11:53:09.415484\n",
      "Extracting data for NO3 for  2025-04-19 11:53:10.148543\n",
      "Extracting data for NO4 for  2025-04-19 11:53:11.378019\n",
      "Extracting data for NO5 for  2025-04-19 11:53:12.159536\n",
      "Extracting data for PL for  2025-04-19 11:53:12.942962\n",
      "Extracting data for SE1 for  2025-04-19 11:53:13.882953\n",
      "Extracting data for SE2 for  2025-04-19 11:53:14.652329\n",
      "Extracting data for SE3 for  2025-04-19 11:53:15.402603\n",
      "Extracting data for SE4 for  2025-04-19 11:53:16.228456\n"
     ]
    }
   ],
   "source": [
    "ep = EpexSpotEnergyPrices()\n",
    "data =  ep.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4440e-9552-4bca-9ed2-771ce9296989",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T05:05:46.984154Z",
     "iopub.status.busy": "2025-04-10T05:05:46.984154Z",
     "iopub.status.idle": "2025-04-10T05:05:47.015402Z",
     "shell.execute_reply": "2025-04-10T05:05:47.015402Z",
     "shell.execute_reply.started": "2025-04-10T05:05:46.984154Z"
    }
   },
   "source": [
    "<a id=\"delta\"></a>\n",
    "# Storing in Delta Tables\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "333753b1-a3ed-42f2-8e3e-c957bb48d1e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T18:53:16.953071Z",
     "iopub.status.busy": "2025-04-18T18:53:16.953071Z",
     "iopub.status.idle": "2025-04-18T18:54:22.600758Z",
     "shell.execute_reply": "2025-04-18T18:54:22.600758Z",
     "shell.execute_reply.started": "2025-04-18T18:53:16.953071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous data exist\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "from delta import *\n",
    "import os\n",
    "import sys\n",
    "from  pyspark.errors import AnalysisException\n",
    "\n",
    "def create_spark_session():\n",
    "    os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "    os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "    os.environ['HADOOP_HOME'] = os.getcwd() + \"\\\\winutils-master\\\\hadoop-3.3.5\\\\\"\n",
    "    os.environ['JAVA_HOME'] = \"C:\\\\Users\\\\gmatharu\\\\Downloads\\\\microsoft_fabric_proj\\\\jdk-8u441-windows-x64\\\\jdk1.8.0_441\\\\\"\n",
    "    os.environ['path'] = os.environ['path'] +';'+ os.getcwd() + '\\\\winutils-master\\\\hadoop-3.3.5\\\\bin\\\\'\n",
    "    os.environ['SPARK_HOME'] = \"C:\\\\Users\\\\gmatharu\\\\Downloads\\\\microsoft_fabric_proj\\\\spark-3.5.5-bin-hadoop3\\\\\"\n",
    "    \n",
    "    builder = pyspark.sql.SparkSession.builder.appName(\"EpexSpotEnergyPrices\")\\\n",
    "              .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\\\n",
    "              .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    try:\n",
    "        spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    except:\n",
    "        print(\"Cannot Create Spark Session, please check your spark installation.Exiting...\")\n",
    "        sys.exit()\n",
    "    return spark\n",
    "def save_delta_table(data):\n",
    "    spark = create_spark_session()\n",
    "    spark_df = spark.createDataFrame(data)\n",
    "    try:\n",
    "        current = spark.read.format(\"delta\").load('epex_spot_prices')\n",
    "        current = current.union(spark_df)\n",
    "        current.write.mode('overwrite').format(\"delta\").save(\"epex_spot_prices\")\n",
    "        print((current.count(), len(current.columns)), (spark_df.count(), len(spark_df.columns)))\n",
    "    except AnalysisException:\n",
    "        print(\"No previous data exist\")\n",
    "        spark_df.write.format(\"delta\").option(\"overwriteSchema\", \"true\").partitionBy(\"market\").save(\"epex_spot_prices\")\n",
    "\n",
    "save_delta_table(data)\n",
    "#.mode('overwrite').option(\"overwriteSchema\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f84b5-abdf-44bb-a7b7-2c776fdad138",
   "metadata": {},
   "source": [
    "<a id=\"schd\"></a>\n",
    "# Scheduling\n",
    "***\n",
    "##### Local\n",
    "***\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "In the next iteration of this project I would use <b>Apache Airlfow</b> for creating mutliple <b>DAGS</b> and scheduling and creating a proper<b>Data Pipeline</b>, but to keep this project manageable, I scheduled the script to run everyday at <b>12:00PM</b> in <b>Window’s Task Scheduler</b>. If you linux feel free to use <b>Cron</b>\n",
    "</div>\n",
    "\n",
    "##### Fabric\n",
    "***\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "I would have created a <b>paramteric Pipeline</b> in <b>Fabric</b> as the code I would have written to extract data would be for a single market. Also, <b>Fabric Pipelines</b> are just <b>Apache Airflow DAGS</b> underneath using a visual designer. So my local work would be parity matched to Fabric.\n",
    "\n",
    "\n",
    "I would have followed below steps:\n",
    "    <ol>\n",
    "        <li>Create a Pipeline</li>\n",
    "        <li>Use the <b>ForEach Activity</b>, since I want to repeat the process for every market\n",
    "        Clicking on the <b>ForEach Activity</b> and then on the Settings tab would allow me to specify the collection I want to loop over. In our case, we want to loop over the markets, so we can use the following expression to build an array of the markets as strings:\n",
    "\n",
    "<code> @createArray('AT','BE','CH','DE-LU','DK1','DK2','FI','FR','GB','NL','NO1','NO2','NO3','NO4','NO5','PL','SE1','SE2','SE3','SE4') </code>\n",
    "\n",
    "To enter the expression,I would open the Pipeline expression builder by clicking on the Add dynamic content link that appears after highlighting the empty text box after Items. The expression builder will open and I would enter the above expression.\n",
    "<br>\n",
    "\n",
    "</li>\n",
    "<li>\n",
    "    Now that I have a loop that goes over the markets, I needed to tell the Pipeline what it should do on every iteration of the loop. Inside the <b>ForEach Activity</b> block, clicking the ✏️ icon would change the canvas to the Activities that are inside the <b>ForEach Activity</b>. \n",
    "</br>\n",
    "Inside this canvas, I would add an Activity to run a Notebook by clicking on the <b>Notebook Activity</b> at the top. Basically, the <b>ForEach Activity</b> would run the <b>Notebook Activity</b> for each market.\n",
    "</li>\n",
    "<li>\n",
    "    In the settings of the <b>Notebook Activity</b> select the Notebook from the dropdown and then expand the <b>Base parameters</b> section and use the <b>+ New button</b> to add a new parameter to the list. <b>The name of the parameter should match the name of the variable in the Notebook</b>.I would set the <b>Type to String</b>. \n",
    "    \n",
    "</br>Now I have to link the parameter value to the loop variable from the <b>ForEach Activity</b> since I would be looping over every market. To do this, I would open the expression builder for this activity and use the following expression:\n",
    "\n",
    "<code>@item()</code>\n",
    "</li>\n",
    "<li>After <b>Saving my work & Validating the pipelinw</b>, I would test it by <b>running it.</b></li>\n",
    "<li>I would Schedule the Pipeline by clicking the <b>Schedule icon</b></li>\n",
    "</ol>\n",
    "\n",
    "<a href=\"https://databear.com/understanding_data_pipelines_microsoft_fabric/\" style=\"color:blue; text-decoration:underline\">\n",
    "REFERENCE 1\n",
    "</a>\n",
    "</br>\n",
    "<a href=\"https://www.red-gate.com/simple-talk/databases/sql-server/bi-sql-server/how-to-build-metadata-driven-pipelines-in-microsoft-fabric/\" style=\"color:blue; text-decoration:underline\">\n",
    "REFERENCE 2\n",
    "</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e592569-da7c-4a20-9874-dd27191dae11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<a id=\"dbt\"></a>\n",
    "# Transformations in DBT\n",
    "***\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "    <b>Note:</b> I am not focussed on modelling for this project.\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "I created a dbt project named <code>eur_enegry_prices</code>\n",
    "</br>\n",
    "<ol>\n",
    "    <li>I would use DBT locally in both local and Fabric environment to creaate data tranformations. I would have also used <b>DataFlow Gen2</b>, but Since I don't have access to Fabric I would prefer DBT.</li>\n",
    "    <li>Setup dbt, get the SQL endpoint from Fabric. Locally, since I would use DuckDB I set up my dbt profile as below\n",
    "</br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src='./images/fabric_project/dbt_profile.png' stlye='display: block; margin: 0 auto' alt=\"Local DBT profile\"  width=\"250\" height=\"300\" >\n",
    "</li>\n",
    "    <li>Check all tranformations I build in my github repo for this project,\n",
    "    <a href=\"https://github.com/gsinghmath\" style=\"color:blue; text-decoration:underline\">here</a>\n",
    "    </br>One of them is <b>Simple Advice</b> suggesting to use electricity intelligently by using home battery by charging it when the price is low and discharging it when the price is high. Will be clear from Power BI Report.\n",
    "</li>\n",
    "    <li>Here are the screenshots of the <b>lineage and the macros and models</b> I built taken from dbt docs website</li>\n",
    "\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "<img src='./images/fabric_project/dbt_project.png' stlye='display: block; margin: 0 auto' alt=\"Local DBT project\"  width=\"800\" height=\"600\"><br>\n",
    "<img src='./images/fabric_project/dbt_database.png' stlye='display: block; margin: 0 auto' alt=\"Local DBT database\"  width=\"800\" height=\"600\" ><br>\n",
    "<img src='./images/fabric_project/dbt_lineage.png' stlye='display: block; margin: 0 auto' alt=\"Local DBT lineage\"  width=\"800\" height=\"600\" >\n",
    "\n",
    "\n",
    "[Back to Top](#0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd4321-2d02-47ac-b63e-ba2dde15bd83",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<a id=\"pbi\"></a>\n",
    "# Power BI Report\n",
    "***\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "    In order to connect Power BI to DuckDB I needed a Custom Connector. I used the connector provided \n",
    "    <a href=\"https://motherduck.com/docs/integrations/bi-tools/powerbi/\" style=\"color:blue; text-decoration:underline\">here</a>.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\" style=\"color:black\">\n",
    "    <b>Note:</b> \n",
    "    </br>I am not focussed on creating a proper dashboard, its just some basic report to showcase an end to end project. Also I cannot use Map Chart as Power BI desktop won't show them for me.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"color:black\">\n",
    "I built four reports showing various stats\n",
    "<ol>\n",
    "    <li><b>Average Prices:</b> showing average <b>Monthly</b> and <b>Daily</b> prices across Countries and Markets.   \n",
    "        </br>\n",
    "        <img src='./images/fabric_project/pbi_avp_prices.png' stlye='display: block; margin: 0 auto' alt=\"Power BI Avg Prices\"  width=\"800\" height=\"600\" >\n",
    "    </li>\n",
    "    <li><b>Price Points and Suggestions Market:</b> Price Stats and Daily suggestions for a chosen market every hour based on price\n",
    "        </br>   \n",
    "        <img src='./images/fabric_project/pbi_suggestion.png' stlye='display: block; margin: 0 auto' alt=\"Power BI Suggestion\"  width=\"800\" height=\"600\" >    \n",
    "    </li>\n",
    "    <li><b>Daily Price Comparison across Markets:</b> Price Comparison across markets for last 7 days\n",
    "        </br>    \n",
    "        <img src='./images/fabric_project/pbi_7days.png' stlye='display: block; margin: 0 auto' alt=\"Power BI 7 days\"  width=\"800\" height=\"600\" > \n",
    "    </li>\n",
    "    <li><b>Price Range Markets:</b> Shows the min vs max price for a chosen market\n",
    "        </br>\n",
    "        <img src='./images/fabric_project/pbi_price_range.png' stlye='display: block; margin: 0 auto' alt=\"Power BI Price Range\"  width=\"800\" height=\"600\" >\n",
    "    </li>\n",
    "</ol>\n",
    "</div>\n",
    "\n",
    "\n",
    "[Back to Top](#0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
